syntax = "proto3";
option go_package = "source.monogon.dev/metropolis/node/core/curator/proto/api";
package metropolis.node.core.curator.proto.api;

import "metropolis/proto/common/common.proto";
import "metropolis/proto/ext/authorization.proto";

// The Curator is the main cluster management service of Metropolis.
//
// It runs on top of Metropolis and is the main entrypoint for both external
// and internal services to get cluster state and and get/mutate cluster
// configuration.
// It is currently implemented as a leader-elected service running on all nodes
// that run a consensus server (etcd). Only the elected leader will serve this
// service.
//
// The curator listens on gRPC on all network interfaces at a well known port,
// with access encrypted and authenticated by TLS using certificates issued by
// the Cluster CA.
//
// The curator is a privileged service, and performs per-RPC authorization based
// on the identity of the client, which is determined by the client certificate
// supplied over TLS.
service Curator {
    // Watch returns a stream of updates concerning some part of the cluster
    // managed by the curator, and is the main way in which node code responds
    // to cluster configuration/state changes.
    // Once open, the Curator will stream WatchEvents pertinent to the
    // requested data. At first, the Curator will send WatchEvent(s) describing
    // the current state of the watched resources, letting the client 'catch
    // up' with the current cluster state. Then, it will stream WatchEvent(s)
    // as the pertinent objects change.
    // There is no way for the client to know whether it is 'up to date' on the
    // object state, as streamed WatchEvents are not synchronous to internal
    // state changes within the Curator. Effectively, the view of Watch clients
    // is eventually consistent with the state of the objects in the Curator.
    rpc Watch(WatchRequest) returns (stream WatchEvent) {
        option (metropolis.proto.ext.authorization) = {
            need: PERMISSION_READ_CLUSTER_STATUS
        };
    }
    // UpdateNodestatus is called by nodes in the cluster to report their own
    // status. This status is recorded by the curator and can be retrieved via
    // Watch.
    rpc UpdateNodeStatus(UpdateNodeStatusRequest) returns (UpdateNodeStatusResponse) {
        option (metropolis.proto.ext.authorization) = {
            need: PERMISSION_UPDATE_NODE_SELF
        };
    }

    // Heartbeat is used by nodes to periodicall update their heartbeat
    // timestamps within the current Curator leader.
    rpc Heartbeat(stream HeartbeatRequest) returns (stream HeartbeatResponse) {
        option (metropolis.proto.ext.authorization) = {
            need: PERMISSION_UPDATE_NODE_SELF
        };
    }

    // RegisterNode is called by nodes that wish to begin registering into the
    // cluster. This will created a 'New' node in the cluster state.
    rpc RegisterNode(RegisterNodeRequest) returns (RegisterNodeResponse) {
        option (metropolis.proto.ext.authorization) = {
            // The node doesn't yet have any credentials and will provide a
            // self-signed ephemeral certificate to prove ownership of an
            // Ed25519 key.
            allow_unauthenticated: true
        };
    }

    // CommitNode is called by nodes that wish to finish registering into
    // the cluster. They must first call RegisterNode, after which they also
    // need to be approved by the cluster (currently, this is done by a manager
    // calling ApproveNode). Then, after performing this call, the node is
    // considered as a fully functioning member of the cluster, and can begin
    // assuming roles assigned to it, report its status, and perform other node
    // functions.
    //
    // The node must perform the call using an ephemeral certificate, in the
    // same way as the RegisterNode call was performed.
    //
    // This call will fail if the given node is not in the STANDBY state, ie.
    // has not yet been approved to join the cluster. It is also _not_
    // idempotent and cannot be retried. In case of a non-transient failure,
    // the calling node should block forever and not retry the full registration
    // process, and instead an administrative intervention (eg. node
    // registration flow restart) should take care of these cases. This is a
    // known limiting factor of the API, but allows for tighter security.
    //
    // This is the point at which the node submits its Cluster Unlock Key, the
    // cluster part of its full disk encryption key. This key will be given to
    // the node whenever it wants to join the cluster again, and the node will
    // combine it with its locally sealed Node Unlock Key to recover the full
    // key.
    //
    // When the RPC is successful, the curator will return the node's newly
    // minted node certificate, which can then be used by the node to perform
    // RPCs acting as this node. This certificate is what the node will use to
    // perform all further communications with the Curator (until a reboot, in
    // which case the join flow requires initial communication using an
    // ephemeral client).
    rpc CommitNode(CommitNodeRequest) returns (CommitNodeResponse) {
        option (metropolis.proto.ext.authorization) = {
            allow_unauthenticated: true
        };
    }

    // JoinNode is called by nodes (re)joining the cluster. Register Flow must
    // be completed beforehand (see: CommitNode). This call will fail if the
    // calling node is not in the UP state. This call is idempotent, and as
    // such it can be retried.
    //
    // JoinNode is authenticated in the transport layer with a Join Key passed
    // to Curator in an earlier RegisterNode call.
    //
    // Upon success, the node will receive its Cluster Unlock Key, enabling it
    // to mount encrypted storage after combining with Node Unlock Key.
    rpc JoinNode(JoinNodeRequest) returns (JoinNodeResponse) {
        option (metropolis.proto.ext.authorization) = {
            allow_unauthenticated: true
        };
    }

    // IssueCertificate issues some TLS certificate (currently only for nodes),
    // effectively performing credential escrow.
    //
    // This is currently used to issue Kubernetes component certificates for
    // nodes (as Kubernetes doesn't understand Metropolis certificates, and we
    // don't want to be running components with node private keys anyway).
    rpc IssueCertificate(IssueCertificateRequest) returns (IssueCertificateResponse) {
        option (metropolis.proto.ext.authorization) = {
        };
    }

    // UpdateNodeClusterNetworking is called by nodes when their local Cluster
    // Networking state changes, ie. they are using a new WireGuard key or have
    // new prefixes to announce. All per-node cluster networking information is
    // then available for other nodes to consume. This information is then used
    // to build up the WireGuard-based cluster networking mesh.
    //
    // The Curator performs basic validation on the submitted prefixes. It makes
    // sure the submitted prefixes are either:
    // 1. fully contained within the cluster's Kubernetes pod network, or
    // 2. a /32 equal to a node's external IP address, which allows communicating
    //    from node IPs to pods.
    //
    // These prefixes are always added to allowedIPs on WireGuard peers, but not
    // automatically added as routes on the nodes. Instead, only the Kubernetes
    // cluster network is programmed into the routing table, which catches all
    // the prefixes of type 1. Prefixes of type 2. are only used to allow
    // incoming traffic from nodes into pods.
    rpc UpdateNodeClusterNetworking(UpdateNodeClusterNetworkingRequest) returns (UpdateNodeClusterNetworkingResponse) {
        option (metropolis.proto.ext.authorization) = {
            need: PERMISSION_UPDATE_NODE_SELF
        };
    }

    // GetConsensusStatus returns the status of the consensus service (etcd)
    // running on curators. This can be used to detect the health of the cluster
    // before operational changes.
    rpc GetConsensusStatus(GetConsensusStatusRequest) returns (GetConsensusStatusResponse) {
        option (metropolis.proto.ext.authorization) = {
            need: PERMISSION_READ_CLUSTER_STATUS
        };
    }
}

// Node is the state and configuration of a node in the cluster.
message Node {
    // ID of the node. Unique across all nodes. Opaque but human-readable.
    string id = 1;
    // Roles that the nodes is supposed to take on.
    metropolis.proto.common.NodeRoles roles = 2;
    // Last reported status of the node, if available.
    metropolis.proto.common.NodeStatus status = 3;
    // Cluster networking configuration/status of node, if available.
    metropolis.proto.common.NodeClusterNetworking clusternet = 4;
    // The node's 'lifecycle' state from the point of view of the cluster.
    metropolis.proto.common.NodeState state = 5;
    metropolis.proto.common.NodeLabels labels = 6;
};

// WatchRequest specifies what data the caller is interested in. This influences
// the contents of WatchEvents.
message WatchRequest {
    // The watcher wants information about a single node within the cluster.
    // This is designed to be used by node-local code that needs to know what
    // the state of the node and the cluster are for purposes of
    // starting/stopping services, performing software updates and general node
    // lifecycle management.
    //
    // If the requested node is not yet present in the cluster, the Watch will
    // block until it is available. If a node is then deleted, a tombstone will
    // be returned and the call Watch will block forever.
    message NodeInCluster {
        // node_id that the watcher is interested in. The curator will, best
        // effort, stream updates (not necessarily all updates) to this node
        // within WatchEvents.
        string node_id = 1;
    }
    // The watcher wants information about all the nodes in the cluster. This
    // is designed to be used by node-local code that needs to know the state
    // of all the nodes within the cluster, for purposes of building aggregate
    // views of the cluster, eg. the addresses of all nodes or a list of nodes
    // fitting some criterion. With time, this call might offer filter
    // functionality to perform some of this filtering server-side.
    message NodesInCluster {
    }
    oneof kind {
        NodeInCluster node_in_cluster = 1;
        NodesInCluster nodes_in_cluster = 2;
    }
}

message WatchEvent {
    // Nodes pertinent to the watch request. The nodes contained might not
    // contain just the nodes requested in WatchRequest, so the client needs to
    // filter out anything spurious.
    repeated Node nodes = 1;
    // Node tombstones, a list of node IDs that have been removed from the
    // cluster since the last sent WatchEvent. For any node in this list, the
    // watcher should perform logic to remove that node from its current state.
    message NodeTombstone {
        string node_id = 1;
    }
    repeated NodeTombstone node_tombstones = 3;

    // Progress of the watch stream. This is set for any event which fulfills
    // some criterion within the context of the watch stream, and is unspecified
    // otherwise.
    enum Progress {
        PROGRESS_UNSPECIFIED = 0;
        // This event contains the last backlogged data from the watcher: all
        // data pertinent to the request that is already known to the server
        // has been returned, and subsequent event receives will block until new
        // data is available. This will be set on exactly one WatchEvent from
        // a NodesInCluster RPC, its behaviour is not defined for other Watch
        // RPCs.
        PROGRESS_LAST_BACKLOGGED = 1;
    }
    Progress progress = 2;
}

message UpdateNodeStatusRequest {
    // node_id is the Metropolis node identity string of the node for which to
    // set a new status. This currently must be the same node as the one
    // performing the RPC and is included for safety.
    string node_id = 1;
    // status to be set. All fields are overwritten.
    metropolis.proto.common.NodeStatus status = 2;
}

message UpdateNodeStatusResponse {
}

message HeartbeatRequest {
}

message HeartbeatResponse {
}

message RegisterNodeRequest {
    // register_ticket is the opaque Register Ticket required from a node to
    // begin registering it into a cluster. It's provided to the registering
    // node by a cluster operator in NodeParameters, and it retrieved by an
    // operator from a running cluster via Management.GetRegisterTicket.
    bytes register_ticket = 1;
    // join_key is an ED25519 public key generated during registration. It's
    // shared with Curator to authenticate the join procedure later on.
    bytes join_key = 2;
    // have_local_tpm is set by the node if it has a local TPM2.0 that it
    // successfully initialized. This information should be verified by the
    // Curator in high-assurance scenarios using hardware attestation.
    bool have_local_tpm = 3;
    metropolis.proto.common.NodeLabels labels = 4;
}

message RegisterNodeResponse {
    // cluster_configuration contains, amongst others, the cluster policies.
    // These can be used by the node to eg. validate whether it is joining a
    // secure cluster or to pick local settings based on said policies.
    metropolis.proto.common.ClusterConfiguration cluster_configuration = 1;
    // tpm_usage tells the node whether it should use its TPM or not.
    metropolis.proto.common.NodeTPMUsage tpm_usage = 2;
    // recommended_node_storage_security tells the node what storage security
    // it should use according to the cluster. However, the node is free to
    // choose any other storage security setting which still fits the cluster
    // node storage policy, eg. due to administrative overrides in startup
    // parameters, available CPU features, etc.
    metropolis.proto.common.NodeStorageSecurity recommended_node_storage_security = 3;
}

message CommitNodeRequest {
    // cluster_unlock_key (CUK) is the cluster part of the local storage full
    // disk encryption key. The node submits it for safekeeping by the cluster,
    // and keeps the local part (node unlock key, NUK) local, sealed by TPM.
    bytes cluster_unlock_key = 1;
    // storage_security is the node storage security setting which the node has
    // implemented as part of its registration flow. The cluster will validate
    // it against its configured policy.
    metropolis.proto.common.NodeStorageSecurity storage_security = 2;
}

message CommitNodeResponse {
    // ca_certificate is the x509 DER-encoded CA certificate for the cluster.
    // The node should use this to verify the cluster identity when connecting
    // to it from this point onward.
    bytes ca_certificate = 1;
    // node_certificate is the x509 DER-encoded certificate of the node, freshly
    // minted by the cluster's curator, signed for the Ed25519 keypair that the
    // node was connecting with. This certificate should be used by the node for
    // communication with the cluster from this point onward.
    bytes node_certificate = 2;
}

message JoinNodeRequest {
    // using_sealed_configuration is set by the node if it has loaded its join
    // keys and NUK from a sealed secret on the EFI system partition. If it did,
    // it means it was previously configured to use the TPM 2.0 to seal it.
    //
    // Naturally, this information should be verified by the Curator
    // in high-assurance scenarios using hardware attestation. Ie., if the node
    // claims to have loaded its keys from the TPM, then it should also be able
    // to prove that it's running using that TPM.
    bool using_sealed_configuration = 1;
}

message JoinNodeResponse {
    // cluster_unlock_key (CUK) is the key submitted by the node through
    // CommitNodeRequest, and returned in this message after authenticating
    // with Join Credentials.
    bytes cluster_unlock_key = 1;
}

// CuratorLocal is served by both the Curator leader and followers, and returns
// data pertinent to the local node or the leader election status of the
// Curator. Most importantly, it can be used to retrieve the current Curator
// leader.
service CuratorLocal {
    // GetCurrentLeader returns the leader known to the contacted curator.
    // An error will be returned if no leader can be established.
    //
    // This is a streaming call so that clients can wait on any changes, instead
    // of polling repeatedly. The server will either reply with new leader
    // information (if available) or close the stream (if not) as early as it's
    // aware of a leadership change.
    rpc GetCurrentLeader(GetCurrentLeaderRequest) returns (stream GetCurrentLeaderResponse) {
        option (metropolis.proto.ext.authorization) = {
            // This call pretty much needs to be public, as it's used in early
            // connections to figure out what curator to connect to. This might
            // be a node which hasn't yet joined a cluster (thereby not having
            // cluster credentials), or it might be a user which hasn't yet
            // authenticated fully into the cluster.
            allow_unauthenticated: true
        };
    }

    // Returns the cluster CA certificate.
    rpc GetCACertificate(GetCACertificateRequest) returns (GetCACertificateResponse) {
        option (metropolis.proto.ext.authorization) = {
            // This call is used by metroctl to perform CA certificate TOFU when
            // the cluster is being taken over. At that point the request is
            // unauthenticated. There's no private data in the certificate, so
            // let's just make it publicly available.
            allow_unauthenticated: true
        };
    }
}

message GetCurrentLeaderRequest {
}

message GetCurrentLeaderResponse {
    // leader_node_id is the leader's (as seen by the responding node) Node ID.
    string leader_node_id = 1;
    // leader_host is the host/IP address at which the leader node's curator
    // is listening.
    //
    // This can be zero/empty if the leader has not yet reported its external
    // address to the cluster.
    //
    // TODO(q3k): guarantee this being always non-zero
    string leader_host = 2;
    // leader_port is the port at which the leader node's curator is listening.
    int32 leader_port = 3;
    // this_node_id is the Node ID of the node which sent this response.
    string this_node_id = 4;
}

message GetCACertificateRequest {
}

message GetCACertificateResponse {
    // DER-encoded (but not PEM armored) certificate of the Kubernetes
    // 'identity' CA, which is used to authenticate services users. The
    // certificates issued below are signed by this CA.
    bytes identity_ca_certificate = 1;
}

message IssueCertificateRequest {
    // Issue a set of TLS certificates for a Kubernetes worker node.
    message KubernetesWorker {
        // The ED25519 public key of the keypair that will run the kubelet and
        // cluster networking.
        //
        // Kubernetes worker certificates can only be issued for one pubkey for
        // a given node. Attempting to retrieve certificates for a different
        // pubkey will fail.
        bytes kubelet_pubkey = 1;
        // The ED25519 public key of the keypair that that will run the CSI provisioner.
        bytes csi_provisioner_pubkey = 2;
        // The ED25519 public key of the keypair that will run nfproxy and clusternet.
        bytes netservices_pubkey = 3;
    }
    oneof kind {
        KubernetesWorker kubernetes_worker = 1;
    };
}

message IssueCertificateResponse {
    message KubernetesWorker {
        // DER-encoded (but not PEM armored) certificate of the Kubernetes
        // 'identity' CA, which is used to authenticate services users. The
        // certificates issued below are signed by this CA.
        bytes identity_ca_certificate = 1;
        // DER-encoded (but not PEM armored) certificate to be used by kubelet
        // when authenticating incoming connections.
        bytes kubelet_server_certificate = 2;
        // DER-encoded (but not PEM armored) certificate to be used by kubelet
        // and cluster networking when connecting to the api server.
        bytes kubelet_client_certificate = 3;
        // DER-encoded (but not PEM armored) certificate to be used by the CSI
        // provisioner when connecting to the api server.
        bytes csi_provisioner_certificate = 4;
        // DER-encoded (but not PEM armored) certificate to be used by worker
        // services nfproxy and clusternet when connecting to the apiserver.
        bytes netservices_certificate = 5;
    }
    oneof kind {
        KubernetesWorker kubernetes_worker = 1;
    };
}

message UpdateNodeClusterNetworkingRequest {
    // Details of the clusternet configuration/state of the node. Whatever is
    // currently configured in the Curatoor will be fully replaced by data
    // contained with this field, after validation.
    metropolis.proto.common.NodeClusterNetworking clusternet = 1;
}

message UpdateNodeClusterNetworkingResponse {
}

message GetConsensusStatusRequest {
}

message GetConsensusStatusResponse {
  // A control plane member as seen by the etcd cluster.
  message EtcdMember {
    // The ID of the etcd member node. This should be the same as the ID of the
    // Metropolis node running this etcd member.
    string id = 1;
    // Status of this node from the point of view of etcd.
    enum Status {
      STATUS_UNKNOWN = 0;
      // The node is an etcd learner - it cannot run the Curator control plane
      // services yet because it's still catching up to the etcd cluster state.
      STATUS_LEARNER = 1;
      // This node is a full etcd member, and can become a Curator control plane
      // leader.
      STATUS_FULL = 2;
    };
    Status status = 2;
  }
  // All members of the etcd cluster.
  repeated EtcdMember etcd_member = 1;
}
